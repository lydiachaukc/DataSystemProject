{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Probabilistic ML Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lydiachaukc/DataSystemProject/blob/main/Probabilistic_ML_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 1.0\n",
        "\n",
        "Last updated: 2022-01-24"
      ],
      "metadata": {
        "id": "J6V_SU_Ec3xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic ML: Assignment 1\n",
        "- **Deadline**: Feb 6, at 23:59.\n",
        "- **Submission**: You need to submit your solutions through Crowdmark, including all your derivations, plots, and your code. You can produce the files however you like (e.g. LATEX, Microsoft Word, etc), as long as it is readable. Points will be deducted if we have a hard time reading your solutions or understanding the structure of your code.\n",
        "- **Collaboration policy**: After attempting the problems on an individual basis, you may discuss and work together on the assignment with up to two classmates. However, **you must write your own code and write up your own solutions individually and explicitly name any collaborators** at the top of the homework.\n"
      ],
      "metadata": {
        "id": "aGZ2cQATe4Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 - Decision Theory"
      ],
      "metadata": {
        "id": "QI9CD9BCoh5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One successful use of probabilistic models is for building spam filters, which take in an email and take different actions depending on the likelihood that it’s spam.\n",
        "\n",
        "Imagine you are running an email service. You have a well-calibrated spam classifier that tells you the probability that a particular email is spam: $p(spam|email)$. You have three options for what to do with each email: You can show it to the user, put it in the spam folder, or delete it entirely.\n",
        "\n",
        "Depending on whether or not the email really is spam, the user will suffer a different amount  of wasted time for the different actions we can take, $L$(action, spam):\n",
        "\n",
        "Action   | Spam        | Not spam\n",
        "-------- | ----------- | -----------\n",
        "Show     | 10          | 0\n",
        "Folder   | 1           | 50\n",
        "Delete   | 0           | 200"
      ],
      "metadata": {
        "id": "lqGwGkzkokTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.1\n",
        "[3pts] Plot the expected wasted user time for each of the three possible actions, as a function of the probability of spam: $p(spam|email)$."
      ],
      "metadata": {
        "id": "2V_k3L8ByUs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rAwFc8cVSXtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = [[10, 0], [1, 50],[0, 200]]\n",
        "actions_names = ['Show', 'Folder', 'Delete']\n",
        "num_actions = len(losses)\n",
        "def expected_loss_of_action(prob_spam, action):\n",
        "    #TODO: Return expected loss over a Bernoulli random variable\n",
        "    # with mean prob_spam.\n",
        "    # Losses are given by the table above.\n",
        "    pass\n",
        "\n",
        "prob_range = np.linspace(0., 1., num=500) \n",
        "\n",
        "# Make plot\n",
        "for action in range(num_actions):\n",
        "    plt.plot(prob_range, expected_loss_of_action(prob_range, action), label=actions_names[action])\n",
        "\n",
        "plt.xlabel('$p(spam|email)$')\n",
        "plt.ylabel('Expected loss of action')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "CwqF1QStyUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.2\n",
        "[2pts] Write a function that computes the optimal action given the probability of spam."
      ],
      "metadata": {
        "id": "oqtMD5X4NZ1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimal_action(prob_spam):\n",
        "    #TODO: return best action given the probability of spam. \n",
        "    #Hint: np.argmin might be helpful.\n",
        "    pass"
      ],
      "metadata": {
        "id": "7gLS3sO1NiPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.3\n",
        "[4pts] Plot the expected loss of the optimal action as a function of the probability of spam.\n",
        "\n",
        "\n",
        "Color the line according to the optimal action for that probability of spam.\n"
      ],
      "metadata": {
        "id": "TaIhABveNinu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_range = np.linspace(0., 1., num=500) \n",
        "optimal_losses = []\n",
        "optimal_actions = []\n",
        "for p in prob_range:\n",
        "    # TODO: Compute the optimal action and its expected loss for\n",
        "    # probability of spam given by p.\n",
        "    pass\n",
        "\n",
        "plt.xlabel('p(spam|email)')\n",
        "plt.ylabel('Expected loss of optimal action')\n",
        "plt.plot(prob_range, optimal_losses)"
      ],
      "metadata": {
        "id": "Gdk3OQLONo-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.4\n",
        "[4pts] For exactly which range of the probabilities of an email being spam should we delete an email?\n",
        "\n",
        "Find the exact answer by hand using algebra."
      ],
      "metadata": {
        "id": "M0eRJyGdNpXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "fr_ghgvoUz5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 - Naïve Bayes, A Generative Model"
      ],
      "metadata": {
        "id": "H1kRdfM6ol0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we'll fit a Naïve Bayes model to the MNIST digits dataset, and use this model for making predictions and generating new images from the same distribution. MNIST is a dataset of 28x28 black-and-white images of handwritten digits of 0 to 9. We represent each image by a vector $x^{(i)} \\in \\{0,1\\}^{784}$, where 0 and 1 represent white and black pixels respectively. Each class label $c^{(i)}$ is a number between 0 and 9, which in the code is represented by a 10-dimensional one-hot vector.\n",
        "\n",
        "![](https://drive.google.com/uc?id=15UvN6-AZj92QDmbl60Q-zR_YTIukx9SO)\n",
        "\n",
        "\n",
        "The Naïve Bayes model parameterized by $\\theta$ and $\\pi$ defines the following joint probability of $x$ and $c$,\n",
        "$$p(x,c|\\theta,\\pi) = p(c|\\pi)p(x|c,\\theta) = p(c|\\pi)\\prod_{j=1}^{784}p(x_j|c,\\theta),$$\n",
        "where $x_j | c,\\theta \\sim \\operatorname{Bernoulli}(\\theta_{jc})$ or in other words $p(x_j | c,\\theta) = \\theta_{jc}^{x_j}(1-\\theta_{jc})^{1-x_j}$, and $c|\\pi$ follows a simple categorical distribution, i.e. $p(c|\\pi) = \\pi_c$.\n",
        "\n",
        "We begin by learning the parameters $\\theta$ and $\\pi$. The following code will download and prepare the training and test sets."
      ],
      "metadata": {
        "id": "Je6H8FAKpqmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import array\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def download(url, filename):\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    out_file = os.path.join('data', filename)\n",
        "    if not os.path.isfile(out_file):\n",
        "        urlretrieve(url, out_file)\n",
        "\n",
        "\n",
        "def mnist():\n",
        "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
        "\n",
        "    def parse_labels(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "    def parse_images(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "    for filename in ['train-images-idx3-ubyte.gz',\n",
        "                     'train-labels-idx1-ubyte.gz',\n",
        "                     't10k-images-idx3-ubyte.gz',\n",
        "                     't10k-labels-idx1-ubyte.gz']:\n",
        "        download(base_url + filename, filename)\n",
        "\n",
        "    train_images = parse_images('data/train-images-idx3-ubyte.gz')\n",
        "    train_labels = parse_labels('data/train-labels-idx1-ubyte.gz')\n",
        "    test_images = parse_images('data/t10k-images-idx3-ubyte.gz')\n",
        "    test_labels = parse_labels('data/t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    return train_images, train_labels, test_images[:1000], test_labels[:1000]\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    partial_flatten = lambda x: np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n",
        "    one_hot = lambda x, k: np.array(x[:, None] == np.arange(k)[None, :], dtype=int)\n",
        "    train_images, train_labels, test_images, test_labels = mnist()\n",
        "    train_images = (partial_flatten(train_images) / 255.0 > .5).astype(float)\n",
        "    test_images = (partial_flatten(test_images) / 255.0 > .5).astype(float)\n",
        "    train_labels = one_hot(train_labels, 10)\n",
        "    test_labels = one_hot(test_labels, 10)\n",
        "    N_data = train_images.shape[0]\n",
        "\n",
        "    return N_data, train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "k587bbiSvhB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.1\n",
        "[2pts] Derive the expression for the Maximum Likelihood Estimator (MLE) of $\\theta$ and $\\pi$."
      ],
      "metadata": {
        "id": "qgGhDuEBvuMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "cISpi3BUOdEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.2\n",
        "[4pts] Using the MLE for this data, many entries of $\\theta$ will be estimated to be 0, which seems extreme. So we look for another estimation method.\n",
        "\n",
        "Assume the prior distribution of $\\theta$ is such that the entries are i.i.d. and drawn from $\\operatorname{Beta}(2,2)$. Derive the Maximum A Posteriori (MAP) estimator for $\\theta$ (it has a simple final form). You can return the MLE for $\\pi$ in your implementation. From now on, we will work with this estimator."
      ],
      "metadata": {
        "id": "gTMSP01Sw-F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "uI3hcFf1Of82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_map_estimator(train_images, train_labels):\n",
        "    \"\"\" Inputs: train_images (N_samples x N_features), train_labels (N_samples x N_classes)\n",
        "        Returns the MAP estimator theta_est (N_features x N_classes) and the MLE\n",
        "        estimator pi_est (N_classes)\"\"\"\n",
        "    \n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "    return theta_est, pi_est"
      ],
      "metadata": {
        "id": "v49Abi0uxeII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.3\n",
        "[5pts] Derive an expression for the class log-likelihood $\\log p(c|x,\\theta,\\pi)$ for a single image. Then, complete the implementation of the following functions. Recall that our prediction rule is to choose the class that maximizes the above log-likelihood, and accuracy is defined as the fraction of samples that are correctly predicted.\n",
        "\n",
        "Report the average log-likelihood $\\frac{1}{N}\\sum_{i=1}^{N}\\log p(c^{(i)}|x^{(i)},\\hat{\\theta},\\hat{\\pi})$ (where $N$ is the number of samples) on the training test, as well the training and test errors."
      ],
      "metadata": {
        "id": "7yO5yq0dyus4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "36FW8dZpOhb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(images, theta, pi):\n",
        "    \"\"\" Inputs: images (N_samples x N_features), theta, pi\n",
        "        Returns the matrix 'log_like' of loglikehoods over the input images where\n",
        "        log_like[i,c] = log p (c |x^(i), theta, pi) using the estimators theta and pi.\n",
        "        log_like is a matrix of (N_samples x N_classes)\n",
        "    Note that log likelihood is not only for c^(i), it is for all possible c's.\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "    return log_like\n",
        "\n",
        "\n",
        "def accuracy(log_like, labels):\n",
        "    \"\"\" Inputs: matrix of log likelihoods and 1-of-K labels (N_samples x N_classes)\n",
        "    Returns the accuracy based on predictions from log likelihood values\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "N_data, train_images, train_labels, test_images, test_labels = load_mnist()\n",
        "theta_est, pi_est = train_map_estimator(train_images, train_labels)\n",
        "\n",
        "loglike_train = log_likelihood(train_images, theta_est, pi_est)\n",
        "avg_loglike = np.sum(loglike_train * train_labels) / N_data\n",
        "train_accuracy = accuracy(loglike_train, train_labels)\n",
        "loglike_test = log_likelihood(test_images, theta_est, pi_est)\n",
        "test_accuracy = accuracy(loglike_test, test_labels)\n",
        "\n",
        "print(f\"Average log-likelihood for MAP is {avg_loglike:.3f}\")\n",
        "print(f\"Training accuracy for MAP is {train_accuracy:.3f}\")\n",
        "print(f\"Test accuracy for MAP is {test_accuracy:.3f}\")"
      ],
      "metadata": {
        "id": "6RZwBnVh0Zoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.4\n",
        "[2pts] In this model, is it always true that any two pixels $x_i$ and $x_j$ with $i \\neq j$ are independent given $c$? How about after marginalizing over $c$? Explain your answers."
      ],
      "metadata": {
        "id": "qFJkXeMK2mwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your answer here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "ImntAmpWOjYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.5 \n",
        "[4pts] Since we have a generative model for our data, we can do more than just prediction. Randomly sample and plot 10 images from the learned distribution over images using the MAP parameter estimates. (Hint: You first need to sample the class $c$, and then sample pixels conditioned on $c$.)"
      ],
      "metadata": {
        "id": "_P4Y1x_G28QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_sampler(theta, pi, num_images):\n",
        "    \"\"\" Inputs: parameters theta and pi, and number of images to sample\n",
        "    Returns the sampled images (N_images x N_features)\"\"\"\n",
        "\n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "    return sampled_images\n",
        "\n",
        "\n",
        "def plot_images(images, ims_per_row=5, padding=5, digit_dimensions=(28, 28),\n",
        "                cmap=matplotlib.cm.binary, vmin=0., vmax=1.):\n",
        "    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n",
        "    fig = plt.figure(1)\n",
        "    fig.clf()\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    N_images = images.shape[0]\n",
        "    N_rows = np.int32(np.ceil(float(N_images) / ims_per_row))\n",
        "    pad_value = vmin\n",
        "    concat_images = np.full(((digit_dimensions[0] + padding) * N_rows + padding,\n",
        "                             (digit_dimensions[1] + padding) * ims_per_row + padding), pad_value)\n",
        "    for i in range(N_images):\n",
        "        cur_image = np.reshape(images[i, :], digit_dimensions)\n",
        "        row_ix = i // ims_per_row\n",
        "        col_ix = i % ims_per_row\n",
        "        row_start = padding + (padding + digit_dimensions[0]) * row_ix\n",
        "        col_start = padding + (padding + digit_dimensions[1]) * col_ix\n",
        "        concat_images[row_start: row_start + digit_dimensions[0],\n",
        "                      col_start: col_start + digit_dimensions[1]] = cur_image\n",
        "        cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    \n",
        "    plt.plot()\n",
        "\n",
        "\n",
        "sampled_images = image_sampler(theta_est, pi_est, 10)\n",
        "plot_images(sampled_images)"
      ],
      "metadata": {
        "id": "amV0qFMC3myy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.6\n",
        "[4pts] One of the advantages of generative models is that they can handle missing data, or be used to answer different sorts of questions about the model. Assume we have only observed some pixels of the image. Let $x_E = \\{x_p : \\text{pixel $p$ is observed}\\}$. Derive an expression for $p(x_j|x_E,\\theta,\\pi)$, the conditional probability of an unobserved pixel $j$ given the observed pixels and distribution parameters. (Hint: You have to marginalize over $c$.)"
      ],
      "metadata": {
        "id": "qCYQ6VVx5YI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Type up your derivation here]\n",
        "\n",
        "Your answer:\n"
      ],
      "metadata": {
        "id": "GGT2yAtLOmJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.7\n",
        "[4pts] We now reveal only a random 20% of the pixels to the model. For the first 20 images in the training set, plot the images when the unobserved pixels are left as white, as well as the same images when the unobserved pixels are filled with the marginal probability of each pixel being 1 given the observed pixels, i.e. the value of the unobserved pixel $j$ is $p(x_j = 1|x_E,\\theta,\\pi)$."
      ],
      "metadata": {
        "id": "5qGf4Rwx6ZPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probabilistic_imputer(theta, pi, original_images, is_observed):\n",
        "    \"\"\"Inputs: parameters theta and pi, original_images (N_images x N_features), \n",
        "        and is_observed which has the same shape as original_images, with a value\n",
        "        1. in every observed entry and 0. in every unobserved entry.\n",
        "    Returns the new images where unobserved pixels are replaced by their \n",
        "    conditional probability\"\"\"\n",
        "    \n",
        "    # YOU NEED TO WRITE THIS PART\n",
        "    return imputed_images\n",
        "\n",
        "\n",
        "num_features = train_images.shape[1]\n",
        "is_observed = np.random.binomial(1, p=0.2, size=(20, num_features))\n",
        "plot_images(train_images[:20] * is_observed)"
      ],
      "metadata": {
        "id": "k8g40hvw6pE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputed_images = probabilistic_imputer(theta_est, pi_est, train_images[:20], is_observed)\n",
        "plot_images(imputed_images)"
      ],
      "metadata": {
        "id": "RtOM8Ba4uVQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Regression [33 pts]\n",
        "\n"
      ],
      "metadata": {
        "id": "AU5LLm4lSN10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Manually Derived Linear Regression [10 pts]"
      ],
      "metadata": {
        "id": "cZHnd7CBVVYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that $X \\in \\mathbb{R}^{m \\times n}$ with $n \\ge m$ and $Y \\in \\mathbb{R}^n$, and that $Y \\sim \\mathcal{N}(X^\\top\\beta, \\sigma^2I)$.\n",
        "\n",
        "In this question you will derive the result that the maximum likelihood estimate $\\hat{\\beta}$ of $\\beta$ is given\n",
        "\n",
        "$$\\hat{\\beta} = (XX^\\top)^{-1}XY$$\n",
        "\n",
        "1. [1 pts] What happens if $n < m$?\n",
        "2. [2 pts] What are the expectation and covariance matrix of $\\hat{\\beta}$ for a given true value of $\\beta$?\n",
        "3. [2 pts] Show that maximizing the likelihood is equivalent to minimizing the squared error $\\sum_{i=1}^n(y_i - x_i\\beta)^2$. [Hint: Use $\\sum_{i=1}^na_i^2 = a^\\top a]$.\n",
        "4. [2 pts] Write the squared error in vector notation and expand. [Hint: Use $\\beta^\\top X^\\top Y = Y^\\top X \\beta$.]\n",
        "5. [3 pts] Use the likelihood expression to write the negative log-likelihood. Write the derivative of the negative log-likelihood with respect to $\\beta$, set equal to zero, and solve to show the maximum likelihood estimate $\\hat{\\beta}$ as above."
      ],
      "metadata": {
        "id": "6WYBbKq-TNh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Toy Data [3 pts]"
      ],
      "metadata": {
        "id": "ouYB-7aQV3jZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: For this question, we will use [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html). JAX is a machine learning framwork with an API almost identical to numpy. JAX has the benefit of implementing automatic differentiation, which we will use later in this question."
      ],
      "metadata": {
        "id": "ylk733yHKwGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For visualization purposes and to minimize computational resources, we will work with 1-dimensional toy data.\n",
        "\n",
        "That is, $X \\in \\mathbb{R}^{m \\times n}$ where $m = 1$. We will learn models for 3 target functions.\n",
        "\n",
        "- `target_f1`, linear trend with constant noise\n",
        "- `target_f2`, linear trend heteroskedastic noise\n",
        "- `target_f3`, non-linear trend with heteroskedatic noise\n",
        "\n"
      ],
      "metadata": {
        "id": "ALuD8MznV6qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jax.scipy.stats as stats\n",
        "from jax.ops import index_update\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import gamma as gammaf\n",
        "from typing import List, Optional, Callable, Tuple\n",
        "%matplotlib inline\n",
        "\n",
        "key = random.PRNGKey(42)"
      ],
      "metadata": {
        "id": "d9c2hKXMWkgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_f1(x, sigma_true):\n",
        "  \"\"\"\n",
        "  Linear trend with constant noise.\n",
        "  \"\"\"\n",
        "  noise = random.normal(key, (x.shape[-1],))\n",
        "  y = 2 * x + sigma_true * noise\n",
        "\n",
        "  return y.flatten()\n",
        "\n",
        "\n",
        "def target_f2(x, **kwargs):\n",
        "  \"\"\"\n",
        "  Linear trend with heteroskedastic noise.\n",
        "  \"\"\"\n",
        "  noise = random.normal(key, (x.shape[-1],))\n",
        "  y = 2 * x + jnp.linalg.norm(x, axis=0) * 0.3 * noise\n",
        "\n",
        "  return y.flatten()\n",
        "\n",
        "\n",
        "def target_f3(x, **kwargs):\n",
        "  \"\"\"\n",
        "  Non-linear trend with heteroskedastic noise.\n",
        "  \"\"\"\n",
        "  noise = random.normal(key, (x.shape[-1],))\n",
        "  y = 2 * x + 5 * jnp.sin(0.5 * x) + jnp.linalg.norm(x, axis=0) * 0.3 * noise\n",
        "\n",
        "  return y.flatten()"
      ],
      "metadata": {
        "id": "7EgxWt_KWgom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [2 pts] Write a function which produces a batch of data $x \\sim \\text{Uniform}(0, 20)$ and $y = target_f(x)$."
      ],
      "metadata": {
        "id": "OXkY1FbBWrc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(target_f: Callable, data_size: int, stdev=0.1):\n",
        "  \"\"\"\n",
        "  Sample data from the target function <target_f> with <data_size> samples.\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "  \n",
        "  ## hint: pass sigma_true as a keyword argument (not positional) to avoid errors\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "9jHwBgjnWqiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = 1, 200    # dim, data_size\n",
        "for target_f in (target_f1, target_f2, target_f3):\n",
        "  x, y = generate_data(target_f, n)\n",
        "  try:\n",
        "    assert x.shape == (m, n)\n",
        "    assert y.shape == (n,)\n",
        "    print(f\"{target_f.__name__} passed\")\n",
        "\n",
        "  except AssertionError:\n",
        "        print(f\"{target_f.__name__} failed\")\n",
        "        print(f\"Expected x to have shape {(m, n)}. Got: {x.shape}\")\n",
        "        print(f\"Expected y to have shape {(n,)}. Got: {y.shape}\")"
      ],
      "metadata": {
        "id": "Up7f6PXEXCCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [1 pt] For all three targets, plot a $n = 1000$ sample of the data."
      ],
      "metadata": {
        "id": "Kf75fF4aW8zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=3, figsize=(10, 10), sharey=True)\n",
        "m, n = 1, 1000\n",
        "\n",
        "for i, target_f in enumerate((target_f1, target_f2, target_f3)):\n",
        "  x, y = # TODO: get data for the right function\n",
        "  axs[i].scatter(x.flatten(), y)\n",
        "  axs[i].set_title(f\"Target function: {target_f.__name__}\")\n",
        "\n",
        "\n",
        "# Plotting code, no need to edit it\n",
        "fig.add_subplot(111, frameon=False)\n",
        "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
        "plt.xlabel(\"Sampled data\")\n",
        "plt.ylabel(\"Sampled targets\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Ax-IhJ7-Woe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Linear Regression with $\\hat{\\beta}$ MLE [4 pts]"
      ],
      "metadata": {
        "id": "FoPEQ9WWXfhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [2 pts] Program the function that computes the maximum likelihood estimate given $X$ and $Y$. Use it to estimate $\\hat{\\beta}$ for a $n=1000$ sample from each target function."
      ],
      "metadata": {
        "id": "6LSa6SqDXn9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_mle(X, Y):\n",
        "  \"\"\"\n",
        "  Return the Beta MLE for the linear regression between X and Y.\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "\n",
        "  return beta"
      ],
      "metadata": {
        "id": "jRJUG2JvXnIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [2 pt] For each function, plot the linear regression model given by $Y \\sim \\mathcal{N}(X^\\top\\beta, \\sigma^2 I)$ for $\\sigma = 1$. This plot should have the line of best fit given by the maximum likelihood estimate, as well as a shaded region around the line corresponding to plus/minus one standard deviation (i.e. the fixed uncertainty $\\sigma = 1.0$). Display 3\n",
        "plots, one for each target function, showing the data sample and maximum likelihood estimate linear regression model fit to that data."
      ],
      "metadata": {
        "id": "LLe718eXYG9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n = 1, 1000\n",
        "\n",
        "for target_f in [target_f1, target_f2, target_f3]:\n",
        "\n",
        "  x, y = # TODO, get data from the target function\n",
        "  beta_estimate = # TODO fit the regression\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(x.flatten(), y, label=\"Generated data\")\n",
        "  yhat = x.transpose() @ beta_estimate\n",
        "  plt.plot(x.flatten(), yhat, label=\"Fitted model\", color=\"red\")\n",
        "\n",
        "  error_up, error_down = yhat - 1, yhat + 1\n",
        "  sorted_indices = lax.top_k(x.flatten(), x.shape[1])[1][::-1]\n",
        "  plt.fill_between(x.flatten()[sorted_indices], \n",
        "                  error_down[sorted_indices], error_up[sorted_indices], \n",
        "                  facecolor='green', alpha=0.4, label=\"Uncertainty\")\n",
        "\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "u1ht0MH4PeGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Log-likelihood of Data Under Model [6 pts]"
      ],
      "metadata": {
        "id": "4tl4X8tF9OpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [2 pts] Write code for the function that computes the likelihood of a scalar $x$ under the Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$. This function should be able to broadcast over $x, \\mu, \\sigma$.  I.e. it should allow these to be arrays of the same shape, and return an array of likelihoods of the same shape, i.e. $x_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i)$."
      ],
      "metadata": {
        "id": "1kIHf5F_9d8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_log_likelihood(mean, stdev, x):\n",
        "  \"\"\"\n",
        "  Compute the gaussian log-likelihood, supporting arguments that are vector or\n",
        "  matrix valued.\n",
        "  \"\"\"\n",
        "  log_pdf = # TODO: implement\n",
        "  return log_pdf"
      ],
      "metadata": {
        "id": "IbN5CVkl9c_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK YOUR SOLUTION IS RIGHT:\n",
        "\n",
        "key1, key2 = random.split(key)\n",
        "x = random.normal(key1)\n",
        "mean = random.normal(key2)\n",
        "stdev = random.uniform(key)\n",
        "log_likelihood = gaussian_log_likelihood(mean, stdev, x)\n",
        "assert log_likelihood.shape == ()\n",
        "assert jnp.isclose(log_likelihood, stats.norm.logpdf(x, mean, stdev))\n",
        "\n",
        "x = random.normal(key1, shape=(100,))\n",
        "mean = random.normal(key2)\n",
        "stdev = random.uniform(key)\n",
        "log_likelihood = gaussian_log_likelihood(mean, stdev, x)\n",
        "assert log_likelihood.shape == (100,)\n",
        "assert all(jnp.isclose(implemented, standard, atol=1e-4, rtol=1e-4) for \n",
        "           implemented, standard in zip(\n",
        "               log_likelihood, stats.norm.logpdf(x, mean, stdev)))\n",
        "\n",
        "x = random.normal(key1, shape=(10,))\n",
        "mean = random.normal(key2, shape=(10,))\n",
        "stdev = random.uniform(key, shape=(10,))\n",
        "log_likelihood = gaussian_log_likelihood(mean, stdev, x)\n",
        "assert log_likelihood.shape == (10,)\n",
        "\n",
        "assert all(jnp.isclose(implemented, standard) for \n",
        "           implemented, standard in zip(\n",
        "               log_likelihood, stats.norm.logpdf(x, mean, stdev)))\n",
        "\n",
        "print(\"All tests passed\")"
      ],
      "metadata": {
        "id": "b-OeCVuR9-Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [2 pts] Use your gaussian log-likelihood function to write a function that computes the negative log-likelihood of the target value $Y$ under the model $Y \\sim \\mathcal{N}(X^\\top\\beta, \\sigma^2 I)$."
      ],
      "metadata": {
        "id": "6Pg3lqBm-ELI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_model_nll(beta, x, y, stdev=1.0):\n",
        "  \"\"\"\n",
        "  Return the negative log-likelihood of y given beta.\n",
        "  \"\"\"\n",
        "  # you can ignore these two lines, they just make sure that scalars are treated as vectors\n",
        "  if beta.ndim is 0:\n",
        "    beta = jnp.array([beta])\n",
        "  \n",
        "  # TODO: implement\n",
        "  pass"
      ],
      "metadata": {
        "id": "z9dizxdu-X1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. [1 pts] Use this function to compute and report the negative log-likelihood of a $n \\in \\{10, 100, 1000\\}$ batch of data under the model with the maximum-likelihood estimate $\\hat{\\beta}$ and $\\sigma \\in \\{0.1, 0.3, 1., 2.\\}$ for each target function."
      ],
      "metadata": {
        "id": "-1x8Vqtn-aPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "for i, target_f in enumerate((target_f1, target_f2, target_f3)):\n",
        "  fname = target_f.__name__\n",
        "  print(f\"------- target_f: {fname} -------\")\n",
        "  for sig_model in (0.1, 0.3, 1.0, 2.0):\n",
        "    print(f\"------- sigma: {sig_model} -------\")\n",
        "\n",
        "    # TODO: compute negative log likelihood\n",
        "    nll = # Hint: This should be your last step\n",
        "  \n",
        "    print(f\"Average Negative Log-Likelihood: {nll.mean()}\")"
      ],
      "metadata": {
        "id": "R5F1SJDj-28U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. [1 pts] For each target function, what is the best choice of $\\sigma$?"
      ],
      "metadata": {
        "id": "oZKKYKzy_AFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE: \n"
      ],
      "metadata": {
        "id": "BbusW6_YTUj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Automatic Differentiation and Maximizing Likelihood\n",
        "\n"
      ],
      "metadata": {
        "id": "cT4SRMNLwKtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a previous question you derived the expression for the derivative of the negative loglikelihood with respect to $\\beta$. We will use that to test the gradients produced by automatic\n",
        "differentiation."
      ],
      "metadata": {
        "id": "Hqwc_Hxr_Ztv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [3 pts] For a random value of $\\beta$, $\\sigma$, and $n = 100$ sample from a target function, use automatic differentiation to compute the derivative of the negative log-likelihood of the sampled data with respect to $\\beta$. Test that this is equivalent to the hand-derived value."
      ],
      "metadata": {
        "id": "enaKIrnr_c77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_nll_grad(x, y, beta, sigma):\n",
        "  \"\"\"\n",
        "  Compute the gradient of the negative log-likelihood (hand-derived)\n",
        "  \"\"\"\n",
        "  # you can ignore these two lines, they just make sure that scalars are treated as vectors\n",
        "  if beta.ndim is 0:\n",
        "    beta = jnp.array([beta])\n",
        "  # TODO\n",
        "  pass\n",
        "\n",
        "\n",
        "def avg_lr_model_nll(beta, x, y, stdev):\n",
        "  return lr_model_nll(beta, x, y, stdev).mean()\n",
        "\n",
        "beta_test = random.normal(key)\n",
        "sigma_test = random.uniform(key, minval=0, maxval=1)\n",
        "x, y = generate_data(target_f1, 100)\n",
        "\n",
        "# grad() uses automatic differentiation\n",
        "ad_grad = grad(avg_lr_model_nll)(beta_test, x, y, sigma_test)\n",
        "hand_derivative = compute_nll_grad(x, y, beta_test, sigma_test)\n",
        "\n",
        "jnp.isclose(ad_grad, hand_derivative)"
      ],
      "metadata": {
        "id": "Hhr_P8Tu_qMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 Train Linear Regression Model with Gradient Descent [5 pts]"
      ],
      "metadata": {
        "id": "UEcjSfEA_rsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question we will compute gradients of negative log-likelihood with respect to $\\beta$. We will use gradient descent to find $\\beta$ that maximizes the likelihood."
      ],
      "metadata": {
        "id": "_HH3e7II_25W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [3 pts] Write a function `train_lin_reg` that accepts a target function and an initial estimate for $\\beta$ and some hyperparameters for batch-size, model variance, learning rate, and number of iterations. Then, for each iteration:\n",
        "\n",
        "- sample data from the target function\n",
        "- compute gradients of negative log-likelihood with respect to $\\beta$ using automatic differentiation\n",
        "- update the estimate of $\\beta$ with gradient descent with specified learning rate \n",
        "\n",
        "and, after all iterations, returns the final estimate of $\\beta$."
      ],
      "metadata": {
        "id": "o8FwfJgGAD0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lin_reg(target_f: Callable, initial_beta, x, y, \n",
        "                  model_variance: float, learning_rate: float, num_iter: int):\n",
        "  \"\"\"\n",
        "  Train the linear regression model using the given parameters.\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "\n",
        "  return beta"
      ],
      "metadata": {
        "id": "BECmc9MNAm58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [2 pts] For each target function, start with an initial parameter $\\beta$, learn an estimate for $\\beta_{\\text{learned}}$ by gradient descent. Then plot a $n = 1000$ sample of the data and the learned linear regression model with shaded region for uncertainty corresponding to plus/minus one standard deviation."
      ],
      "metadata": {
        "id": "xi2uLsS4Ana1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for target_function in [target_f1, target_f2, target_f3]:\n",
        "  \n",
        "  x, y = # TODO: get the right data\n",
        "  intial_beta = # TODO sample a random beta\n",
        "  fitted_beta = # TODO fit the linear regression\n",
        "\n",
        "  # Plotting code (you don't need to edit it)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  x, y = generate_data(target_function, 1000, random.PRNGKey(1), 1.0)\n",
        "  plt.scatter(x.flatten(), y, label=\"Generated data\")\n",
        "  yhat = x.transpose() @ fitted_beta\n",
        "  plt.plot(x.flatten(), yhat, label=\"Fitted model\", color=\"red\")\n",
        "\n",
        "  error_up, error_down = yhat - 1, yhat + 1\n",
        "  \n",
        "  sorted_indices = lax.top_k(x.flatten(), x.shape[1])[1][::-1]\n",
        "  plt.fill_between(x.flatten()[sorted_indices], \n",
        "                  error_down[sorted_indices], error_up[sorted_indices], \n",
        "                  facecolor='green', alpha=0.4, label=\"Uncertainty\")\n",
        "\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "uQV0eEelk_RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 Non-linear regression with a Neural Network [5 pts]\n",
        "\n",
        "In the previous questions we have considered a linear regression model \n",
        "\n",
        "$$ Y \\sim \\mathcal{N}(X^\\top \\beta, \\sigma^2 I) $$\n",
        "\n",
        "This model specified the mean of the prediction distribution for each distribution by the product of that datapoint with our parameter.\n",
        "\n",
        "Now, let us generalize this to consider a model where the mean of the predictive distribution\n",
        "is a non-linear function of each datapoint. We will have our non-linear model be a simple\n",
        "function called neural net with parameters $\\theta$ (collection of weights and biases).\n",
        "\n",
        "$$Y \\sim \\mathcal{N}(\\text{neural_net}(X, \\theta), \\sigma^2)$$"
      ],
      "metadata": {
        "id": "OUVDKoXOBVqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for a fully-connected neural network (multi-layer perceptron) with one 10-dimensional hidden layer and a tanh nonlinearirty. You must write this yourself using only basic operations like matrix multiply and tanh, you may not use layers provided by a library. This network will output the mean vector, test that it outputs the correct shape for some random parameters."
      ],
      "metadata": {
        "id": "mH1KUGHsB05Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(x, theta):\n",
        "  \"\"\"\n",
        "  Compute the forward pass of a neural network with 1 hidden layer consisting\n",
        "  of 10 hidden neurons and a tanh nonlinear activation.\n",
        "  \"\"\"\n",
        "  n = x.shape[-1]\n",
        "  input_to_hidden_weights = theta[\"input_hidden_w\"]\n",
        "  hidden_to_output_weights = theta[\"hidden_out_w\"]\n",
        "  input_to_hidden_bias = theta[\"input_hidden_b\"]\n",
        "  hidden_to_output_bias = theta[\"hidden_out_b\"]\n",
        "\n",
        "\n",
        "  outputs = jnp.tanh(x.transpose() @ input_to_hidden_weights + \n",
        "                     input_to_hidden_bias)\n",
        "  \n",
        "  return outputs @ hidden_to_output_weights + hidden_to_output_bias"
      ],
      "metadata": {
        "id": "gxGGDtQOCBOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "x, y = generate_data(target_f1, n)\n",
        "\n",
        "theta = {\n",
        "    \"input_hidden_w\": random.normal(key, shape=(1, 10)),\n",
        "    \"hidden_out_w\": random.normal(key, shape=(10,)),\n",
        "    \"input_hidden_b\": random.normal(key, shape=(10,)),\n",
        "    \"hidden_out_b\": random.normal(key, shape=(1,))\n",
        "}\n",
        "\n",
        "mean = neural_network(x, theta)\n",
        "assert mean.shape == (n,)"
      ],
      "metadata": {
        "id": "_gJeNQ2lCKFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [1pts] Write the code that computes the negative log-likelihood for this model where the mean is given by the output of the neural network and $\\sigma = 1.0$"
      ],
      "metadata": {
        "id": "TUX-YfaYCByo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_model_nll(theta, x, y, sigma=1.0):\n",
        "  \"\"\"\n",
        "  Compute the negative log-likelihood for the model where the mean is the output\n",
        "  of the neural network.\n",
        "  \"\"\"\n",
        "  mean = # TODO get neural network output\n",
        "  stdev_arg = jnp.ones(y.flatten().shape) * sigma\n",
        "\n",
        "  return -gaussian_log_likelihood(mean, stdev_arg, y)"
      ],
      "metadata": {
        "id": "uTWGW4RQCG9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. [2 pts] Write a function `train_nn_reg` that accepts a target function and an initial estimate for $\\theta$ and some hyperparameters for batch-size, model variance, learning rate, and number of iterations. Then, for each iteration:\n",
        "\n",
        "- sample data from the target function\n",
        "- compute gradients of negative log-likelihood with respect to $\\theta$\n",
        "- update the estimate of $\\theta$ with gradient descent with specified learning rate\n",
        "\n",
        "and, after all iterations, returns the final estimate of $\\theta$."
      ],
      "metadata": {
        "id": "u4V75qmVCNo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_nn_model_nll(theta, x, y, sigma=1.0):\n",
        "  return nn_model_nll(theta, x, y, sigma).mean()\n",
        "  \n",
        "def train_nn_reg(target_f: Callable, initial_theta, x, y,\n",
        "                 model_variance: float = 1.0, learning_rate: float= 1e-3, num_iter: int=5000\n",
        "                 ):\n",
        "  \"\"\"\n",
        "  Train the linear regression model using the given parameters.\n",
        "  \"\"\"\n",
        "  theta = initial_theta\n",
        "  \n",
        "  sigma = model_variance ** (1/2)\n",
        "\n",
        "  log_frequency = int(jnp.ceil(0.1 * num_iter))\n",
        "\n",
        "  model_nll_grad = grad(avg_nn_model_nll)\n",
        "  \n",
        "  for i in range(num_iter):\n",
        "    avg_neg_log_like = # TODO: compute average negative log likelihood\n",
        "    if i % log_frequency == 0:\n",
        "      print(f\"[Iteration {i + 1}] Loss: {avg_neg_log_like}\")\n",
        "\n",
        "    grad_theta = # TODO: compute gradient here\n",
        "    # Hint: use the grad() function\n",
        "    \n",
        "    theta[\"input_hidden_w\"] -= learning_rate * grad_theta[\"input_hidden_w\"]\n",
        "    theta[\"hidden_out_w\"] -= learning_rate * grad_theta[\"hidden_out_w\"]\n",
        "    theta[\"input_hidden_b\"] -= learning_rate * grad_theta[\"input_hidden_b\"]\n",
        "    theta[\"hidden_out_b\"] -= learning_rate * grad_theta[\"hidden_out_b\"]\n",
        "\n",
        "  return theta"
      ],
      "metadata": {
        "id": "LD5pVwRGCrXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. [2pts] For each target function, start with an initialization of the network parameters, $\\theta$, use your train function to minimize the negative log-likelihood and find an estimate for $\\theta$ learned by gradient descent. Then plot a $n = 1000$ sample of the data and the learned regression model with shaded uncertainty bounds given by $\\sigma = 1.0$."
      ],
      "metadata": {
        "id": "WKH4tBBtCsn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "initial_theta = {\n",
        "    \"input_hidden_w\": random.normal(key, shape=(1, 10)),\n",
        "    \"hidden_out_w\": random.normal(key, shape=(10,)),\n",
        "    \"input_hidden_b\": random.normal(key, shape=(10,)),\n",
        "    \"hidden_out_b\": random.normal(key, shape=(1,))\n",
        "}\n",
        "\n",
        "for target_function in [target_f1, target_f2, target_f3]:\n",
        "\n",
        "  x, y = # TODO: get the right data\n",
        "  fitted_params = # TODO: fit a neural network to your data\n",
        "  \n",
        "  # Plotting code, you don't need to edit this\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  x, y = generate_data(target_f1, 1000, 1.0)\n",
        "  plt.scatter(x.flatten(), y, label=\"Generated data\")\n",
        "  plot_x = np.linspace(0, 20, 1000)\n",
        "  nn_preds = neural_network(plot_x.reshape(1, 1000), fitted_params)\n",
        "  plt.plot(plot_x, nn_preds, label=\"Fitted model\", color=\"red\")\n",
        "\n",
        "  error_up, error_down = nn_preds + 1, nn_preds - 1\n",
        "  plt.fill_between(plot_x, \n",
        "                  error_down, error_up, \n",
        "                  facecolor='green', alpha=0.2, label=\"Uncertainty\")\n",
        "\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "sTTQoXvYXOX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}